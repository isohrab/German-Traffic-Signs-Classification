{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import data\n",
    "from skimage import transform\n",
    "from skimage.filters import sobel\n",
    "from skimage.color import rgb2gray\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read data from file\n",
    "# set data path\n",
    "train_path = \"data/Trainset/\"\n",
    "test_path = \"data/Testset/Online-Test/Images\"\n",
    "test_csv_path = \"data/Testset/Online-Test/GT-online_test.csv\"\n",
    "\n",
    "## first read training data\n",
    "# get all folders\n",
    "directories = [d for d in os.listdir(train_path)\n",
    "              if os.path.isdir(os.path.join(train_path, d))]\n",
    "\n",
    "train_images = []\n",
    "labels_tr = []\n",
    "# loop over all folders and get the image names\n",
    "for d in directories:\n",
    "    current_dir = os.path.join(train_path, d)\n",
    "    file_names = [os.path.join(current_dir, f)\n",
    "                   for f in os.listdir(current_dir)\n",
    "                   if f.endswith(\".ppm\")]\n",
    "    for f in file_names:\n",
    "        train_images.append(data.imread(f))\n",
    "        labels_tr.append(int(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images\n",
    "file_names = [os.path.join(test_path, f)\n",
    "            for f in os.listdir(test_path)\n",
    "            if f.endswith(\".ppm\")]\n",
    "\n",
    "# sort file names to be equal of labels in CSV files\n",
    "file_names.sort()\n",
    "\n",
    "test_images = []\n",
    "for f in file_names:\n",
    "    test_images.append(data.imread(f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test set labels from csv file\n",
    "test_labels_csv = read_csv(test_csv_path,sep=';')\n",
    "test_labels = test_labels_csv.ClassId[:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## resize all images to 32 by 32 pixel\n",
    "# Image size:\n",
    "image_size = 32\n",
    "# number of channel\n",
    "num_channel = 2  # grayscale + sobel\n",
    "# Resizing\n",
    "images_tr = [transform.resize(i,(32,32), mode='constant') for i in train_images]\n",
    "images_ts = [transform.resize(i,(32,32), mode='constant') for i in test_images]\n",
    "# convert to grayscale\n",
    "images_tr = [rgb2gray(i) for i in images_tr]\n",
    "images_ts = [rgb2gray(i) for i in images_ts]\n",
    "\n",
    "# get sobel filters\n",
    "sobel_tr = [sobel(i) for i in images_tr]\n",
    "sobel_ts = [sobel(i) for i in images_ts]\n",
    "\n",
    "# Concatenate sobel and grayscale as a two channel image\n",
    "images_tr = np.concatenate([images_tr, sobel_tr],-1)\n",
    "images_ts = np.concatenate([images_ts, sobel_ts],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test data into two dataset and validation set\n",
    "# to be sure we have to work with right data length\n",
    "#assert len(test_images)==len(test_labels)\n",
    "\n",
    "# as we can see, the length of labels and images are not equal!!\n",
    "# we will use images length which are smaller than labels\n",
    "m = len(images_ts)\n",
    "# create a shuffle list\n",
    "per = np.arange(m)\n",
    "np.random.shuffle(per)\n",
    "\n",
    "# remove the last labels\n",
    "test_labels = test_labels[0:m]\n",
    "\n",
    "# convert images list to np.array\n",
    "test_images_np = np.array(images_ts)\n",
    "\n",
    "# shuffle testset before dividing\n",
    "test_images_shuffled = test_images_np[per]\n",
    "test_labels_shuffled = test_labels[per]\n",
    "\n",
    "# half of the length\n",
    "half = int(m/2)\n",
    "\n",
    "# first half = testset\n",
    "images_ts = test_images_shuffled[0:half]\n",
    "labels_ts = test_labels_shuffled[0:half]\n",
    "\n",
    "# second half = devset\n",
    "images_dv = test_images_shuffled[half:m]\n",
    "labels_dv = test_labels_shuffled[half:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini batch generator\n",
    "def mini_batch_generator(x, y, mini_batch_size):\n",
    "    '''\n",
    "    Get data and shuffle the indices then divide data into mini_batch_size \n",
    "    '''\n",
    "    assert len(x)==len(y)\n",
    "    \n",
    "    # length of input data\n",
    "    m = len(x)\n",
    "    \n",
    "    # create a shuffled list of indices\n",
    "    idx = np.random.permutation(m)\n",
    "    \n",
    "    # shuffle data\n",
    "    x_shuffle = x[idx]\n",
    "    y_shuffle = y[idx]\n",
    "    \n",
    "    # calculate number of mini batches\n",
    "    num_mini_batches = int(m/mini_batch_size)\n",
    "    \n",
    "    # create a list to hold mini batches\n",
    "    mini_batches = []\n",
    "    \n",
    "    for i in range(num_mini_batches):\n",
    "        # take i-th mini batch from shuffled data\n",
    "        x_batch = x_shuffle[i*mini_batch_size:i*mini_batch_size + mini_batch_size]\n",
    "        y_batch = y_shuffle[i*mini_batch_size:i*mini_batch_size + mini_batch_size]\n",
    "        batch = (x_batch, y_batch)\n",
    "        # append batch to list of mini batches\n",
    "        mini_batches.append(batch)\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(y, num_classes=None):\n",
    "    '''\n",
    "    get a list of numbers and convert it to one-hot vectors based number of classes\n",
    "    '''\n",
    "    # if num_classes is not provided, we can find it based on unique numbers in the list\n",
    "    if num_classes is None:\n",
    "        num_classes = len(set(y))\n",
    "    # this is trick! thank to the stackoverflow!\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the length of number of classes\n",
    "num_classes = len(set(labels_tr))\n",
    "# Convert labels to one-hot vectors\n",
    "train_one_hot_labels = one_hot_encoder(labels_tr, num_classes)\n",
    "devset_one_hot_labels = one_hot_encoder(labels_dv, num_classes)\n",
    "testset_one_hot_labels = one_hot_encoder(labels_ts, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First set hyper parameters\n",
    "kernel_size = [3, 3, 3, 3]\n",
    "filter_depths = [16 , 32, 32, 64]\n",
    "max_pooling = [False, True, False, True]\n",
    "mini_batch_size = 64\n",
    "lr = 0.001\n",
    "lr_decay = 0.95\n",
    "fc_hidden = 512\n",
    "fc_layers = 2\n",
    "dropout = 0.5\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial inputs placeholder\n",
    "# input images\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, image_size, image_size, num_channel], name='X')\n",
    "# input labels\n",
    "Y_true = tf.placeholder(dtype=tf.float32, shape=[None, num_classes], name='Y')\n",
    "\n",
    "Y_true_cls = tf.argmax(Y_true, 1)\n",
    "\n",
    "keep_prob = tf.Variable(initial_value=0.5, dtype=tf.float32, name='keep_prob', trainable=False)\n",
    "\n",
    "learning_rate = tf.Variable(initial_value=0.001, dtype=tf.float32, name=\"learning_rate\", trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 conv2d: (?, 32, 32, 16)\n",
      "layer 1 conv2d: (?, 16, 16, 32)\n",
      "layer 2 conv2d: (?, 16, 16, 32)\n",
      "layer 3 conv2d: (?, 8, 8, 64)\n"
     ]
    }
   ],
   "source": [
    "## define CNN layers\n",
    "cnn_input = X\n",
    "assert len(kernel_size)==len(filter_depths)==len(max_pooling)\n",
    "\n",
    "for i in range(len(kernel_size)):\n",
    "    cnn_input = tf.layers.conv2d(cnn_input, \n",
    "                                 filters=filter_depths[i], \n",
    "                                 kernel_size=kernel_size[i], \n",
    "                                 padding='same', \n",
    "                                 activation=tf.nn.relu)\n",
    "    # max pooling only on layers which are True\n",
    "    if max_pooling[i]:\n",
    "        cnn_input = tf.layers.max_pooling2d(cnn_input, pool_size=2, strides=2)\n",
    "    \n",
    "    # print what happen to layers! :)\n",
    "    print(\"layer {} conv2d: {}\".format(i, cnn_input.get_shape()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define fully connected layer\n",
    "fc_input = tf.contrib.layers.flatten(cnn_input)\n",
    "for i in range(fc_layers):\n",
    "    fc_input = tf.contrib.layers.fully_connected(fc_input, fc_hidden)\n",
    "    fc_input = tf.nn.dropout(fc_input, keep_prob=keep_prob)\n",
    "    fc_hidden = int(fc_hidden/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute logits and cost\n",
    "logits = tf.contrib.layers.fully_connected(fc_input, num_classes, activation_fn=None)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimize cost with adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict labels\n",
    "y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(y_pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate accuracy\n",
    "correct_prediction = tf.equal(y_pred_cls, Y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert train_images to np.array\n",
    "images_tr_np = np.array(images_tr)\n",
    "# reshape images to [mini_batch_size, image_size, image_size, num_channel]\n",
    "images_tr_np = np.reshape(images_tr_np, [-1, image_size, image_size, num_channel])\n",
    "\n",
    "images_dv_np = np.array(images_dv)\n",
    "# reshape images to [mini_batch_size, image_size, image_size, num_channel]\n",
    "images_dv_np = np.reshape(images_dv_np, [-1, image_size, image_size, num_channel])\n",
    "\n",
    "\n",
    "# Save best model\n",
    "save_dir = \"checkpoints/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "save_path = os.path.join(save_dir, 'GTSC')\n",
    "# initial tensorflow global variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# create a saver object\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    # run global initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    # create a list to store train cost and validation cost for further plot\n",
    "    epoch_costs = []\n",
    "    validation_costs = []\n",
    "    \n",
    "    # store best accuracy over the batches\n",
    "    best_acc = 0.\n",
    "\n",
    "    # start computation\n",
    "    tic = time.time()\n",
    "    \n",
    "    # Loop over number of epochs\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        train_losses = 0.\n",
    "        # generate mini bachtes in each epoch to generate each time differently\n",
    "        mini_batches = mini_batch_generator(images_tr_np, train_one_hot_labels, mini_batch_size=mini_batch_size)\n",
    "\n",
    "        # Loop over train mini_batches\n",
    "        for b in mini_batches:\n",
    "            # get the X,Y batches\n",
    "            x_batch, y_batch = b\n",
    "            loss, _ = sess.run([cost, optimizer], feed_dict={X:x_batch,\n",
    "                                                           Y_true: y_batch,\n",
    "                                                            learning_rate:lr,\n",
    "                                                           keep_prob:dropout})\n",
    "            train_losses += loss\n",
    "            \n",
    "        # add epoch_loss to train cost\n",
    "        epoch_costs.append(train_losses)\n",
    "        \n",
    "        # decay learning rate with lr_decay\n",
    "        lr = lr * lr_decay\n",
    "        \n",
    "        ## evaluate over devset\n",
    "        # Generate devset mini batches\n",
    "        devset_mini_batches = mini_batch_generator(images_dv_np, \n",
    "                                                   devset_one_hot_labels, \n",
    "                                                   mini_batch_size=mini_batch_size)\n",
    "        \n",
    "        # check accuracy over trained model\n",
    "        dev_accs = []\n",
    "        dev_losses = 0.\n",
    "        \n",
    "        # Loop over devset mini batches\n",
    "        for b in devset_mini_batches:\n",
    "            \n",
    "            x_batch, y_batch = b\n",
    "            \n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={X:x_batch,\n",
    "                                                           Y_true: y_batch,\n",
    "                                                           keep_prob:1.0})\n",
    "            dev_accs.append(acc)\n",
    "            dev_losses += loss\n",
    "        \n",
    "        # Print a report of current epoch:\n",
    "        print(\"Epoch:{0:>3} train cost: {1:>6.2f} dev cost:{2:>6.2f} dev accuracy:{3:>4.2%}\".format(e+1,\n",
    "                                                                                                   train_losses,\n",
    "                                                                                                   dev_losses,\n",
    "                                                                                                   np.mean(dev_accs)))\n",
    "        \n",
    "        # save model if achieved better accuracy\n",
    "        if best_acc < np.mean(dev_accs):\n",
    "            saver.save(sess, save_path=save_path)\n",
    "        \n",
    "    # Finish computation\n",
    "    toc = time.time()\n",
    "    print(\"Total time of training: \" + str(timedelta(seconds=int(toc-tic))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert testset to np.array\n",
    "images_ts_np = np.array(images_ts)\n",
    "# expand channel dimension\n",
    "images_ts_np = np.expand_dims(images_ts_np, -1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore previous trained model\n",
    "    saver.restore(sess, save_path=save_path)\n",
    "    \n",
    "    # test only first 1000 images \n",
    "    x_batch = images_ts_np[0:1000]\n",
    "    y_batch = testset_one_hot_labels[0:1000]\n",
    "    \n",
    "    pred_results, pred_cls, true_cls, acc, test_los = sess.run([correct_prediction, \n",
    "                                                                y_pred_cls, \n",
    "                                                                Y_true_cls, \n",
    "                                                                accuracy, \n",
    "                                                                cost],\n",
    "                                                                feed_dict={X:x_batch,\n",
    "                                                                        Y_true: y_batch,\n",
    "                                                                        keep_prob:1.0})\n",
    "    print(\"test cost: {0:>4.4f}, accuracy: {1:>4.2%}\".format(test_los, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show false answers\n",
    "incorrect = (pred_results==False)    # len(incorrect)=1000\n",
    "images = x_batch[incorrect]          # len(images) = total number of false predicted e.g 41 (of 1000)\n",
    "# remove back channel dimension\n",
    "images = np.squeeze(images)\n",
    "false_predict = pred_cls[incorrect]  # len(false_predict) == len(images)\n",
    "\n",
    "true_labels = true_cls[incorrect]    # len(true_labels)== len(false_predict)==len(images)\n",
    "\n",
    "# true_list = labels_tr\n",
    "assert len(images)==len(false_predict)\n",
    "m = len(images)\n",
    "plt.figure(figsize=(12,4*m))\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "counter = 1\n",
    "\n",
    "for i in range(m):\n",
    "    # this is what model has been false predicted\n",
    "    plt.subplot(m,6, counter)\n",
    "    plt.imshow(images[i], cmap=plt.cm.gray)\n",
    "    plt.title(\"{}\\n true label:{}\".format(i, true_labels[i]))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # this is the real label\n",
    "    plt.subplot(m,6,counter+1)\n",
    "    plt.imshow(images_tr[labels_tr.index(false_predict[i])], cmap=plt.cm.gray)\n",
    "    plt.title(\"{}\\n predicted label:{}\".format(i, false_predict[i]))\n",
    "    plt.axis('off')\n",
    "    counter +=2\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
